{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **웹 크롤링**\n",
    "Web Mining\n",
    "## **1 Natural Language Processing**\n",
    "자연어 분석을 위한 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir, moviedict = './data/Movie/movie/', {}\n",
    "\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f  = open(moviehtmldir + '/' + filename, encoding=\"ISO-8859-1\")\n",
    "    parsed_html   = BeautifulSoup(f.read(), \"lxml\")\n",
    "    try:    title = parsed_html.body.h1.text       \n",
    "    except: title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "tknzr    = WordPunctTokenizer()\n",
    "\n",
    "def ListDocs(dirname):\n",
    "    docs, titles = [], []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f    = open(dirname+'/'+filename,'r')\n",
    "        id_  = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id_])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir_ = './data/Movie/txt_sentoken/'\n",
    "pos_textreviews, pos_titles = ListDocs(dir_ + 'pos/')\n",
    "neg_textreviews, neg_titles = ListDocs(dir_ + 'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles      = pos_titles + neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def PreprocessTfidf(texts,stoplist=[],stem=False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "            tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        else:\n",
    "            tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "vectorizer        = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews,stoplist,True)\n",
    "mod_tfidf         = vectorizer.fit(processed_reviews)\n",
    "vec_tfidf         = mod_tfidf.transform(processed_reviews)\n",
    "tfidf             = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump tf-idf into file\n",
    "import pickle\n",
    "#print mod_tfidf.get_feature_names()\n",
    "print(len(processed_reviews),'--',len(mod_tfidf.get_feature_names()))\n",
    "v = mod_tfidf.transform(processed_reviews)\n",
    "\n",
    "with open('./data/vectorizer.pk', 'wb') as fin:\n",
    "      pickle.dump(mod_tfidf, fin)\n",
    "\n",
    "file = open(\"./data/vectorizer.pk\",'rb')\n",
    "load_tfidf =  pickle.load(file)\n",
    "load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test LSA\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "class GenSimCorpus(object):\n",
    "    \n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts      = texts\n",
    "        self.stoplist   = stoplist\n",
    "        self.stem       = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem:\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                yield (x for x in tknzr.tokenize(text) if x not in stoplist)\n",
    "\n",
    "corpus      = GenSimCorpus(tot_textreviews,stoplist,True)\n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics     = 10\n",
    "lsi         = models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U     = lsi.projection.u\n",
    "Sigma = np.eye(ntopics) * lsi.projection.s\n",
    "\n",
    "# calculate V\n",
    "V          = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Review = namedtuple('Review','words tags')\n",
    "dir_ = './data/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "filelist = [f   for f in os.listdir(dir_ + 'pos/') \n",
    "                if str(f)[0] != '.']\n",
    "\n",
    "for filename in filelist:\n",
    "    f = open(dir_ + 'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir_ + 'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir_ + 'neg/' + filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "cores     = multiprocessing.cpu_count()\n",
    "vec_size  = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, vec_size=vec_size, window=10, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#train\n",
    "numepochs= 20\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "\n",
    "# for epoch in range(numepochs):\n",
    "#     try:\n",
    "#         print('epoch %d' % (epoch))\n",
    "#         model_d2v.train(tot_reviews)\n",
    "#         model_d2v.train(tot_reviews, epochs=model.iter, total_examples=model.corpus_count)\n",
    "#         model_d2v.alpha *= 0.99\n",
    "#         model_d2v.min_alpha = model_d2v.alpha\n",
    "#     except (KeyboardInterrupt, SystemExit):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query\n",
    "query = ['science','future','action']\n",
    "#similar tfidf\n",
    "#sparse matrix so the metrics transform into regular vectors before computing cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_vec  = mod_tfidf.transform(PreprocessTfidf([' '.join(query)],stoplist,True))\n",
    "sims       = cosine_similarity(query_vec,vec_tfidf)[0]\n",
    "indxs_sims = sims.argsort()[::-1]\n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print ('sim:',sims[d],' title:',tot_titles[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSA query\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(dict_words.keys()))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q\n",
    "\n",
    "q    = TransformWordsListtoQueryVec(query,dict_words,True)\n",
    "qk   = np.dot(np.dot(q,U),Sigma)\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d] = np.dot(qk,V[d])\n",
    "\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print('sim:',sims[d],' doc:',tot_titles[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #doc2vec query\n",
    "# #force inference to get the same result\n",
    "# model_d2v.random = np.random.RandomState(1)\n",
    "# query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))\n",
    "\n",
    "# model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "# reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5)\n",
    "# for review in reviews_related:\n",
    "#     print('relevance:',review[1],'  title:',tot_titles[review[0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Post Processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#get titles\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir = './data/Movie/movie/'\n",
    "moviedict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id_ = filename.split('.')[0]\n",
    "    f = open(moviehtmldir + '/' + filename, encoding=\"ISO-8859-1\")\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "        title = parsed_html.body.h1.text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    moviedict[id_] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r', encoding=\"ISO-8859-1\")\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir_ = './data/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir_ + 'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir_ + 'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenSimCorpus(object):\n",
    "    \n",
    "    def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "        self.texts, self.stem, self.stoplist, self.bestwords = texts, stem, stoplist, bestwords\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem: yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                if len(self.bestwords)>0: yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                else: yield (x for x in tknzr.tokenize(text) if x not in stoplist)                      \n",
    "\n",
    "num_topics = 10\n",
    "corpus     = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda   = corpus.dictionary\n",
    "lda        = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "lda.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#filter out very common words like mobie and film or very unfrequent terms\n",
    "out_ids  = [tokenid for tokenid, docfreq in dict_lda.dfs.items() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "corpus   = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]\n",
    "\n",
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)\n",
    "for t in range(num_topics):\n",
    "    print('topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import namedtuple\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PreprocessReviews(text,stop=[],stem=False):\n",
    "    #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words title tags')\n",
    "dir_   = './data/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "\n",
    "for filename in [f for f in os.listdir(dir_ +'pos/') if str(f)[0]!='.']:\n",
    "    f   = open(dir_+'pos/'+filename,'r', encoding=\"ISO-8859-1\")\n",
    "    id_ = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id_],['pos_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir_ +'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir_ + 'neg/' + filename,'r')\n",
    "    id_ = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id_],['neg_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_reviews = reviews_pos + reviews_neg\n",
    "#split in test training sets\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (portionpos,'-',portionneg)\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print (len(trainfeatures))\n",
    "testfeatures = negfeatures[portionneg:]  + posfeatures[portionpos:]\n",
    "#shuffle(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "#training naive bayes \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##testing\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print ('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "\n",
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams       = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos  = int(len(posfeatures)*0.8)\n",
    "portionneg  = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(portionpos,'-',portionneg) \n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures)) \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    "\n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "tot_words = pos_words + neg_words\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}\n",
    "\n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "print('total: ',len(word_scores)) \n",
    "best = sorted(word_scores.items(), reverse=True)[:10000] # key=lambda (w,s): s\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "## test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, vec_size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tot_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "# train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print('epoch %d' % (epoch))\n",
    "        model_d2v.train(it, epochs=model.iter, total_examples=model.corpus_count)\n",
    "#         model_d2v.train(tot_reviews)\n",
    "#         model_d2v.alpha *= 0.99\n",
    "#         model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "train_d2v    = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "'accuracy:',classifier.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "'accuracy:',clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svm linear\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
