{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **웹 크롤링**\n",
    "Web Mining\n",
    "## **1 Natural Language Processing**\n",
    "자연어 분석을 위한 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir, moviedict = './data/Movie/movie/', {}\n",
    "\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f  = open(moviehtmldir + '/' + filename, encoding=\"ISO-8859-1\")\n",
    "    parsed_html   = BeautifulSoup(f.read(), \"lxml\")\n",
    "    try:    title = parsed_html.body.h1.text       \n",
    "    except: title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "tknzr    = WordPunctTokenizer()\n",
    "\n",
    "def ListDocs(dirname):\n",
    "    docs, titles = [], []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f    = open(dirname+'/'+filename,'r')\n",
    "        id_  = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id_])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir_ = './data/txt_sentoken/'\n",
    "pos_textreviews, pos_titles = ListDocs(dir_ + 'pos/')\n",
    "neg_textreviews, neg_titles = ListDocs(dir_ + 'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles      = pos_titles + neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def PreprocessTfidf(texts,stoplist=[],stem=False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "            tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        else:\n",
    "            tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "vectorizer        = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews,stoplist,True)\n",
    "mod_tfidf         = vectorizer.fit(processed_reviews)\n",
    "vec_tfidf         = mod_tfidf.transform(processed_reviews)\n",
    "tfidf             = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 -- 39516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x39516 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dump tf-idf into file\n",
    "import pickle\n",
    "#print mod_tfidf.get_feature_names()\n",
    "print(len(processed_reviews),'--',len(mod_tfidf.get_feature_names()))\n",
    "v = mod_tfidf.transform(processed_reviews)\n",
    "\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "      pickle.dump(mod_tfidf, fin)\n",
    "\n",
    "file = open(\"vectorizer.pk\",'rb')\n",
    "load_tfidf =  pickle.load(file)\n",
    "load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test LSA\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "class GenSimCorpus(object):\n",
    "    \n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts      = texts\n",
    "        self.stoplist   = stoplist\n",
    "        self.stem       = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem:\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                yield (x for x in tknzr.tokenize(text) if x not in stoplist)\n",
    "\n",
    "corpus      = GenSimCorpus(tot_textreviews,stoplist,True)\n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics     = 10\n",
    "lsi         = models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django111/lib/python3.6/site-packages/gensim/matutils.py:491: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n"
     ]
    }
   ],
   "source": [
    "U     = lsi.projection.u\n",
    "Sigma = np.eye(ntopics) * lsi.projection.s\n",
    "\n",
    "# calculate V\n",
    "V          = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Review = namedtuple('Review','words tags')\n",
    "dir_ = './data/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "filelist = [f   for f in os.listdir(dir_ + 'pos/') \n",
    "                if str(f)[0] != '.']\n",
    "\n",
    "for filename in filelist:\n",
    "    f = open(dir_ + 'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir_ + 'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir_ + 'neg/' + filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "cores     = multiprocessing.cpu_count()\n",
    "vec_size  = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, vec_size=vec_size, window=10, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#train\n",
    "numepochs= 20\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "\n",
    "# for epoch in range(numepochs):\n",
    "#     try:\n",
    "#         print('epoch %d' % (epoch))\n",
    "#         model_d2v.train(tot_reviews)\n",
    "#         model_d2v.train(tot_reviews, epochs=model.iter, total_examples=model.corpus_count)\n",
    "#         model_d2v.alpha *= 0.99\n",
    "#         model_d2v.min_alpha = model_d2v.alpha\n",
    "#     except (KeyboardInterrupt, SystemExit):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 0.1779486504573889  title: No Telling (1991)\n",
      "sim: 0.1778211465665258  title: Total Recall (1990)\n",
      "sim: 0.17378379866060237  title: Time Machine, The (1960)\n",
      "sim: 0.16303179622390185  title: Bicentennial Man (1999)\n",
      "sim: 0.160582512878302  title: Andromeda Strain, The (1971)\n"
     ]
    }
   ],
   "source": [
    "#query\n",
    "query = ['science','future','action']\n",
    "#similar tfidf\n",
    "#sparse matrix so the metrics transform into regular vectors before computing cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_vec  = mod_tfidf.transform(PreprocessTfidf([' '.join(query)],stoplist,True))\n",
    "sims       = cosine_similarity(query_vec,vec_tfidf)[0]\n",
    "indxs_sims = sims.argsort()[::-1]\n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print ('sim:',sims[d],' title:',tot_titles[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim: 4.030303923978979  doc: Star Wars: Episode I - The Phantom Menace (1999)\n",
      "sim: 3.4188592993754714  doc: Alien³ (1992)\n",
      "sim: 3.4087585167111683  doc: Rocky Horror Picture Show, The (1975)\n",
      "sim: 2.995740915887351  doc: Starship Troopers (1997)\n",
      "sim: 2.8594701640674547  doc: Wild Things (1998)\n"
     ]
    }
   ],
   "source": [
    "#LSA query\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(dict_words.keys()))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q\n",
    "\n",
    "q    = TransformWordsListtoQueryVec(query,dict_words,True)\n",
    "qk   = np.dot(np.dot(q,U),Sigma)\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d] = np.dot(qk,V[d])\n",
    "\n",
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print('sim:',sims[d],' doc:',tot_titles[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #doc2vec query\n",
    "# #force inference to get the same result\n",
    "# model_d2v.random = np.random.RandomState(1)\n",
    "# query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))\n",
    "\n",
    "# model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "# reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5)\n",
    "# for review in reviews_related:\n",
    "#     print('relevance:',review[1],'  title:',tot_titles[review[0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Post Processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#get titles\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir = './data/Movie/movie/'\n",
    "moviedict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id_ = filename.split('.')[0]\n",
    "    f = open(moviehtmldir + '/' + filename, encoding=\"ISO-8859-1\")\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "        title = parsed_html.body.h1.text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    moviedict[id_] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r', encoding=\"ISO-8859-1\")\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir_ = './data/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir_ + 'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir_ + 'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"movie\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"even\" + 0.000*\"get\"'),\n",
       " (1,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"even\" + 0.000*\"get\" + 0.000*\"time\"'),\n",
       " (2,\n",
       "  '0.716*\" \" + 0.037*\"\\n\" + 0.022*\"\\'\" + 0.011*\"-\" + 0.002*\"film\" + 0.002*\"movie\" + 0.001*\"one\" + 0.001*\"like\" + 0.001*\"even\" + 0.001*\"good\"'),\n",
       " (3,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"movie\" + 0.000*\"like\" + 0.000*\"get\" + 0.000*\"time\"'),\n",
       " (4,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"film\" + 0.000*\"like\" + 0.000*\"get\" + 0.000*\"even\"'),\n",
       " (5,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"one\" + 0.000*\"movie\" + 0.000*\"like\" + 0.000*\"get\" + 0.000*\"good\"'),\n",
       " (6,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"even\" + 0.000*\"like\" + 0.000*\"time\"'),\n",
       " (7,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"black\" + 0.000*\"cauldron\" + 0.000*\"`\" + 0.000*\"taran\"'),\n",
       " (8,\n",
       "  '0.000*\" \" + 0.000*\"\\n\" + 0.000*\"\\'\" + 0.000*\"-\" + 0.000*\"film\" + 0.000*\"movie\" + 0.000*\"one\" + 0.000*\"like\" + 0.000*\"even\" + 0.000*\"plot\"'),\n",
       " (9,\n",
       "  '0.734*\" \" + 0.037*\"\\n\" + 0.011*\"\\'\" + 0.006*\"-\" + 0.003*\"film\" + 0.002*\"one\" + 0.002*\"movie\" + 0.001*\"like\" + 0.001*\"time\" + 0.001*\"story\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenSimCorpus(object):\n",
    "    \n",
    "    def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "        self.texts, self.stem, self.stoplist, self.bestwords = texts, stem, stoplist, bestwords\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem: yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                if len(self.bestwords)>0: yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                else: yield (x for x in tknzr.tokenize(text) if x not in stoplist)                      \n",
    "\n",
    "num_topics = 10\n",
    "corpus     = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda   = corpus.dictionary\n",
    "lda        = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "lda.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django111/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0   words:  0.005*\"see\" + 0.004*\"bad\" + 0.004*\"/\" + 0.003*\"really\" + 0.003*\"could\" + 0.003*\"star\" + 0.003*\"+\" + 0.003*\"plot\" + 0.003*\"films\" + 0.003*\"characters\"\n",
      "topic  1   words:  0.004*\"characters\" + 0.004*\"best\" + 0.003*\"scene\" + 0.003*\"life\" + 0.003*\"could\" + 0.003*\"great\" + 0.003*\"action\" + 0.003*\"performance\" + 0.002*\"role\" + 0.002*\"scenes\"\n",
      "topic  2   words:  0.004*\"characters\" + 0.003*\"life\" + 0.003*\"really\" + 0.003*\"films\" + 0.003*\"vampire\" + 0.002*\"many\" + 0.002*\"scenes\" + 0.002*\"see\" + 0.002*\"seems\" + 0.002*\"people\"\n",
      "topic  3   words:  0.003*\"man\" + 0.003*\"life\" + 0.003*\"little\" + 0.003*\"characters\" + 0.003*\"love\" + 0.002*\"best\" + 0.002*\"many\" + 0.002*\"people\" + 0.002*\"&\" + 0.002*\"scenes\"\n",
      "topic  4   words:  0.003*\"life\" + 0.003*\"family\" + 0.003*\"man\" + 0.002*\"best\" + 0.002*\"see\" + 0.002*\"characters\" + 0.002*\"films\" + 0.002*\"world\" + 0.002*\"go\" + 0.002*\"people\"\n",
      "topic  5   words:  0.006*\"star\" + 0.005*\"trek\" + 0.004*\"funny\" + 0.003*\"action\" + 0.003*\"people\" + 0.003*\"work\" + 0.003*\"=\" + 0.003*\"scene\" + 0.002*\"seen\" + 0.002*\"little\"\n",
      "topic  6   words:  0.010*\"/\" + 0.004*\"10\" + 0.004*\"comedy\" + 0.004*\"really\" + 0.004*\"bad\" + 0.003*\"plot\" + 0.003*\"films\" + 0.003*\"characters\" + 0.003*\"funny\" + 0.003*\"scene\"\n",
      "topic  7   words:  0.004*\"alien\" + 0.003*\"smith\" + 0.003*\"see\" + 0.003*\"really\" + 0.003*\"scene\" + 0.003*\"plot\" + 0.003*\"many\" + 0.003*\"little\" + 0.003*\"us\" + 0.003*\"back\"\n",
      "topic  8   words:  0.004*\"characters\" + 0.003*\"films\" + 0.003*\"really\" + 0.003*\"know\" + 0.003*\"see\" + 0.003*\"/\" + 0.003*\"scene\" + 0.003*\"people\" + 0.003*\"could\" + 0.003*\"little\"\n",
      "topic  9   words:  0.006*\"`\" + 0.003*\"see\" + 0.003*\"little\" + 0.003*\"plot\" + 0.003*\"characters\" + 0.003*\"/\" + 0.003*\"john\" + 0.003*\"bad\" + 0.003*\"new\" + 0.002*\"life\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#filter out very common words like mobie and film or very unfrequent terms\n",
    "out_ids  = [tokenid for tokenid, docfreq in dict_lda.dfs.items() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "corpus   = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]\n",
    "\n",
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)\n",
    "for t in range(num_topics):\n",
    "    print('topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import namedtuple\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PreprocessReviews(text,stop=[],stem=False):\n",
    "    #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words title tags')\n",
    "dir_   = './data/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "\n",
    "for filename in [f for f in os.listdir(dir_ +'pos/') if str(f)[0]!='.']:\n",
    "    f   = open(dir_+'pos/'+filename,'r', encoding=\"ISO-8859-1\")\n",
    "    id_ = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id_],['pos_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir_ +'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir_ + 'neg/' + filename,'r')\n",
    "    id_ = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id_],['neg_'+str(cnt)]))\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_reviews = reviews_pos + reviews_neg\n",
    "#split in test training sets\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "print (portionpos,'-',portionneg)\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print (len(trainfeatures))\n",
    "testfeatures = negfeatures[portionneg:]  + posfeatures[portionpos:]\n",
    "#shuffle(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "#training naive bayes \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##testing\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print ('error rate: ',err/float(len(testfeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle\n",
    "\n",
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams       = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos  = int(len(posfeatures)*0.8)\n",
    "portionneg  = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(portionpos,'-',portionneg) \n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures)) \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    "\n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "tot_words = pos_words + neg_words\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}\n",
    "\n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "print('total: ',len(word_scores)) \n",
    "best = sorted(word_scores.items(), reverse=True)[:10000] # key=lambda (w,s): s\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print(portionpos,'-',portionneg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print(len(trainfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "## test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print('test on: ',len(testfeatures)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print('error rate: ',err/float(len(testfeatures))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, vec_size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tot_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "# train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print('epoch %d' % (epoch))\n",
    "        model_d2v.train(it, epochs=model.iter, total_examples=model.corpus_count)\n",
    "#         model_d2v.train(tot_reviews)\n",
    "#         model_d2v.alpha *= 0.99\n",
    "#         model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "train_d2v    = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "'accuracy:',classifier.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "'accuracy:',clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svm linear\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
