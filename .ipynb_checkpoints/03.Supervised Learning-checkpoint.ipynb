{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Supervised Learning***\n",
    "## **1 모델의 평가**\n",
    "- bias : 예측 결과의 **편향 (알고리즘의 가정의 문제로** 발생)\n",
    "- variance : 분산 (포인트 $X_1$ 에 대한 잘못 예측된 레이블)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **01 Mean Square Error (MSE : 평균 제곱 오차)**\n",
    "- 오차 제곱의 평균\n",
    "- 자율학습 방법의 평가척도 $ MSE(\\sigma^-) ={1 \\over N} \\sum d(y_t-y_t^p)^2  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02 Generalized linear models (일반화 선형모델)**\n",
    "- **Function ($E_i는 모델의 오차$)** : $X$는 1개 cf) $ y_i = \\sum \\sigma_j x_j^i + E_i  = h_\\sigma(x^i) + E_i $\n",
    "- **Cost :** Stochastic Gradient Descent(확률 내리막 경사법)는 $\\sigma_i$ 값에 따라 최종값 근처의 진폭이 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03 linear regression (선형회귀)**\n",
    "- 선형 모델 회귀식 : '선형예측함수'로 회귀식 모델링, 알려지지 않은 파라미터는 데이터로 부터 추정합니다\n",
    "<p> $ h_\\sigma(x^i) = \\sigma_0 + \\sigma_1x_1^i + \\sigma_2x_2^i +...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04 ridge regression (리지회귀)** \n",
    "- **L1 정규화 제약에** 활용되어 **W를 0으로** 제한합니다\n",
    "- **비용함수에 \"정규화 항\"을 추가,** 모델에 제약(Constraint)을 부과\n",
    "- 과적합 모델을 방지 $ J^` = { 1 \\over 2}\\sum (y_i - h_\\sigma(x^i))^2 + { \\Gamma \\over 2}\\sum_j^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **05 Lasso regression(라소 회귀)**\n",
    "- **L2 정규화 제약에** 활용되어 **W 기울기를 최소로** 합니다\n",
    "- 정규화 항이 **파라미터 절대값의 합만** 인 부분을 제외한 나머지는 **'04.리지회귀'** 와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **06 Logistic regression(로지스틱 회귀)**\n",
    "- 분류를 위한 **'조건부 확률'로** 로지스틱 함수를 사용\n",
    "- 로지스틱 활용시, **1) 0,1 분류** 또는 **2) cross-entropy로 다중분류가** 가능합니다\n",
    "\n",
    "#### **1) function**\n",
    "- Target 값이 범주형 (0,1) 의 값\n",
    "- $ h_\\sigma(x^i) $ 는 0과 1 사이의 연속된 값을 갖는다  : $ h_\\sigma(x^i) $ 는 확률적 분류기(Probabilistic  classifier)\n",
    "<p>$ h_\\sigma(x^i) = \\begin{matrix} > 0.5 : 1\\\\ < 0.5 : 0\\\\\\end{matrix}$</p>\n",
    "\n",
    "#### **2) Cost function** (일반화 선형 모델에 대한 확률적 해석)\n",
    "- 우도를 최대화 하는 것이, 비용함수를 최소화 한다\n",
    "- 내리막 경사법 함께 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **07 K-근접 이웃법**\n",
    "- 숫자형 데이터간의 거리를 측정\n",
    "- 민코스키 방법(유클리드, 맨해튼 함수를 일반화한 공식) </br>\n",
    "<p>$ (\\sum(|x_j^k - x_j^t|)^q )^{1\\over q}$    ...cf) q = 1 (맨해튼) , 2 (유클리드), 무한대 (슈프림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **08 나이브 베이즈 (베이지안 정리)**\n",
    "- 나이브 베이즈 (베이지안 정리) : 사후확률을 최대화 해주는 레이블 $P$를 찾는다\n",
    "- $ P = argmax  P(y|x_0, x_1..... x_M)  $  - 종속변수 y 가 최대가 되는 독립변수 x를 찾는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **09 의사결정트리**\n",
    "- **Decision Node** : 결정 노드 (2개 이상의 가지가 있고, 의사결정이 나뉠 떄)\n",
    "- **Leaf Node** : 잎 노드 (데이터를 분류하는 노드)\n",
    "- 최적의 분할 규칙 : **불순도 함수 $I$를 최소화** 하는 분류 $ cf) (t_k^q, q) = argmin.I(t_k^j, j) $\n",
    "- **Cost 함수** \n",
    "    1. **Entropy** (엔트로피) :  $ H(S_b) = - \\sum P_b \\log_2 P_b $\n",
    "    2. **gini impurity** (지니불순도) : $ H(S_b) = - \\sum P_b (1- P_b) $\n",
    "    3. **misclassification** (오분류) : $ H(S_b) = 1 - max(P_b) $\n",
    "    4. **mean squared error** (평균제곱오차) : $ H(S_b) = { 1 \\over N_b} \\sum (y_i - \\sigma_b)^2 $ 회귀분석에서 사용\n",
    "- 집합이 클경우 일반화가 잘 안되는 단점 (차원 축소가 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10 서포트 벡터 머신**\n",
    "- **결정경계 거리를 최대로 하여,** 여백을 최대로 하는 $w, b$의 값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11 커널 트릭**\n",
    "- 데이터 집합이 선형으로 분리되지 않을 때, **다른 차원의 공간으로 매핑** 합니다.\n",
    "- 커널함수 (2차원 분리가능) $ K(x^i, x^j) = e^{-|x^i-x^j|^2 \\over 2\\sigma^2} $\n",
    "    - Kernel의 종류 : **liner, RBF Basis, Polynomial, Sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 방법간의 비교**\n",
    "- Cross Validation 절차에 따라 모델을 평가 합니다\n",
    "  1. **k-1개 폴드를 훈련집합으로** 모델을 훈련\n",
    "  2. **나머지 1 폴드로 테스트**\n",
    "  3. 시작점에 정한 폴드 **갯수 k만큼 반복**\n",
    "  4. 정확도는 k번 반복하여, **정확도의 평균을 계산**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 Regression Problem**\n",
    "### **01 회귀 모델의 평가**\n",
    "회귀문제 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn import cross_validation (통합)\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read_CSV & shuffle the data\n",
    "df = pd.read_csv('./data/housing.data',delim_whitespace=True ,header=None)\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "X  = df[df.columns[:-1]].values\n",
    "Y  = df[df.columns[-1]].values\n",
    "cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression -----  \n",
      "mean R2: 0.70 (+/- 0.26) \n",
      "MSE: 23.582354718454805\n"
     ]
    }
   ],
   "source": [
    "# 선형회귀 모델의 평가 \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics      import mean_squared_error\n",
    "lin       = LinearRegression()\n",
    "scores    = model_selection.cross_val_score(lin, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lin, X,Y, cv=cv)\n",
    "print ('linear regression ----- ',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression ------  \n",
      "mean R2: 0.70 (+/- 0.26) \n",
      "MSE: 23.719274751516757\n"
     ]
    }
   ],
   "source": [
    "# Ridge 회귀 모델의 평가\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge     = Ridge(alpha=1.0)\n",
    "scores    = model_selection.cross_val_score(ridge, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(ridge, X,Y, cv=cv)\n",
    "print ('ridge regression ------ ',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression \n",
      "mean R2: 0.69 (+/- 0.28) \n",
      "MSE: 24.730165252239782\n"
     ]
    }
   ],
   "source": [
    "# Lasso 회귀 모델의 평가\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso     = Lasso(alpha=0.1)\n",
    "scores    = model_selection.cross_val_score(lasso, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lasso, X,Y, cv=cv)\n",
    "print ('lasso regression',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree regression \n",
      "mean R2: 0.71 (+/- 0.30) \n",
      "MSE: 22.34873517786561\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree 모델의 평가\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree      = DecisionTreeRegressor(random_state=0)\n",
    "scores    = model_selection.cross_val_score(tree, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(tree, X,Y, cv=cv)\n",
    "print ('decision tree regression',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ValueError: min_samples_split must be at least 2 or in (0, 1], got 1\n",
    "#-------------------------------------------------------------------- \n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# forest = RandomForestRegressor(n_estimators=50, max_depth=Non샤e,min_samples_split=1, \n",
    "#                                random_state=0)\n",
    "# scores = model_selection.cross_val_score(forest, X, Y, cv=cv)\n",
    "# predicted = model_selection.cross_val_predict(forest, X,Y, cv=cv)\n",
    "# print ('random forest regression',\n",
    "#        \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "#        '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02 SVM 모델의 평가**\n",
    "Support Vector Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear support vector machine \n",
      "mean R2: -27.93 (+/- 58.04) \n",
      "MSE: 2090.5283269339247\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import svm\n",
    "svm_lin   = svm.SVR(epsilon=0.2, kernel='linear', C=1, max_iter=1000)\n",
    "scores    = model_selection.cross_val_score(svm_lin, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print ('linear support vector machine',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support vector machine rbf \n",
      "mean R2: -0.01 (+/- 0.12) \n",
      "MSE: 83.90031841018478\n"
     ]
    }
   ],
   "source": [
    "clf       = svm.SVR(epsilon=0.2,kernel='rbf',C=1., max_iter=1000)\n",
    "scores    = model_selection.cross_val_score(clf, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "print ('support vector machine rbf',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn \n",
      "mean R2: 0.49 (+/- 0.29) \n",
      "MSE: 38.435412648221345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn       = KNeighborsRegressor()\n",
    "scores    = model_selection.cross_val_score(knn, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(knn, X,Y, cv=cv)\n",
    "print ('knn',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03 재귀적 특징 축소방법 (Recursive Feature Elimination Method)**\n",
    "- **가장 큰 절대 가중치를 갖는 속성을** 고려하여, **원하는 갯수의 특징을 선택할때 까지 반복**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on linear regression \n",
      "R2: 0.57 (+/- 0.39) \n",
      "MSE: 33.49885000851894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "best_features = 4\n",
    "rfe_lin   = RFE(lin,best_features).fit(X,Y)\n",
    "mask      = np.array(rfe_lin.support_)\n",
    "scores    = model_selection.cross_val_score(lin, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lin, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on linear regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection ridge regression \n",
      "R2: 0.57 (+/- 0.40) \n",
      "MSE: 33.57947297228479\n"
     ]
    }
   ],
   "source": [
    "rfe_ridge = RFE(ridge,best_features).fit(X,Y)\n",
    "mask      = np.array(rfe_ridge.support_)\n",
    "scores    = model_selection.cross_val_score(ridge, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(ridge, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection ridge regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on lasso regression \n",
      "R2: 0.65 (+/- 0.29) \n",
      "MSE: 27.392472831385437\n"
     ]
    }
   ],
   "source": [
    "rfe_lasso = RFE(lasso,best_features).fit(X,Y)\n",
    "mask      = np.array(rfe_lasso.support_)\n",
    "scores    = model_selection.cross_val_score(lasso, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lasso, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on lasso regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on decision tree \n",
      "R2: 0.67 (+/- 0.30) \n",
      "MSE: 25.412312252964426\n"
     ]
    }
   ],
   "source": [
    "rfe_tree  = RFE(tree,best_features).fit(X,Y)\n",
    "mask      = np.array(rfe_tree.support_)\n",
    "scores    = model_selection.cross_val_score(tree, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(tree, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on decision tree',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on linear support vector machine \n",
      "R2: 0.56 (+/- 0.42) \n",
      "MSE: 2090.5283269339247\n"
     ]
    }
   ],
   "source": [
    "rfe_svm   = RFE(svm_lin,best_features).fit(X,Y)\n",
    "mask      = np.array(rfe_svm.support_)\n",
    "scores    = model_selection.cross_val_score(svm_lin, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print ('feature selection on linear support vector machine',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ValueError: min_samples_split must be at least 2 or in (0, 1], got 1\n",
    "# # -------------------------------------------------------------------- \n",
    "\n",
    "# rfe_forest = RFE(forest,best_features).fit(X,Y)\n",
    "# mask       = np.array(rfe_forest.support_)\n",
    "# scores     = model_selection.cross_val_score(forest, X[:,mask], Y, cv=cv)\n",
    "# predicted  = model_selection.cross_val_predict(forest, X[:,mask],Y, cv=cv)\n",
    "# print ('feature selection on random forest',\n",
    "#        \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "#        '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 Classification Problem**\n",
    "분류의 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1  2  3      4     5      6\n",
       "0  vhigh  vhigh  2  2  small   low  unacc\n",
       "1  vhigh  vhigh  2  2  small   med  unacc\n",
       "2  vhigh  vhigh  2  2  small  high  unacc"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read data in\n",
    "df = pd.read_csv('./data/car.data', delimiter=',' ,header=None)\n",
    "for i in range(len(df.columns)):\n",
    "    df[i] = df[i].astype('category')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6\n",
       "630  0  0  3  1  2  1  2\n",
       "824  0  1  2  1  1  0  0\n",
       "398  3  1  2  2  2  0  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map catgories to values\n",
    "map0 = dict( zip( df[0].cat.categories, range( len(df[0].cat.categories ))))\n",
    "map1 = dict( zip( df[1].cat.categories, range( len(df[1].cat.categories ))))\n",
    "map2 = dict( zip( df[2].cat.categories, range( len(df[2].cat.categories ))))\n",
    "map3 = dict( zip( df[3].cat.categories, range( len(df[3].cat.categories ))))\n",
    "map4 = dict( zip( df[4].cat.categories, range( len(df[4].cat.categories ))))\n",
    "map5 = dict( zip( df[5].cat.categories, range( len(df[5].cat.categories ))))\n",
    "map6 = dict( zip( df[6].cat.categories, range( len(df[6].cat.categories ))))\n",
    "\n",
    "cat_cols     = df.select_dtypes(['category']).columns\n",
    "df[cat_cols] = df[cat_cols].apply(lambda x: x.cat.codes)\n",
    "df           = df.iloc[np.random.permutation(len(df))]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_f1        = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_precision = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_recall    = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def CalcMeasures(method, y_pred, y_true, df_f1 = df_f1,\\\n",
    "                 df_precision = df_precision,\n",
    "                 df_recall    = df_recall):\n",
    "    df_f1.loc[len(df_f1)]               = [method]+list(f1_score(y_pred,y_true,average=None))\n",
    "    df_precision.loc[len(df_precision)] = [method]+list(precision_score(y_pred,y_true,average=None))\n",
    "    df_recall.loc[len(df_recall)]       = [method]+list(recall_score(y_pred,y_true,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "#from sklearn import cross_validation (통합)\n",
    "\n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values\n",
    "\n",
    "cv     = 10\n",
    "method = 'linear support vector machine'\n",
    "clf    = svm.SVC(kernel='linear',C=50, max_iter=1000)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'rbf support vector machine'\n",
    "clf = svm.SVC(kernel='rbf',C=50, max_iter=1000)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'poly support vector machine'\n",
    "clf = svm.SVC(kernel='poly',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "method = 'decision tree'\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "method = 'random forest'\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=0,max_features=None)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "method = 'naive bayes'\n",
    "clf = MultinomialNB()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "method = 'logistic regression'\n",
    "clf = LogisticRegression()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "method = 'k nearest neighbours'\n",
    "clf = KNeighborsClassifier(weights='distance',n_neighbors=5)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.315508</td>\n",
       "      <td>0.052830</td>\n",
       "      <td>0.573491</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.994805</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.999173</td>\n",
       "      <td>0.992248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.781915</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.935510</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.961340</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.961832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.956410</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.990437</td>\n",
       "      <td>0.961240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.825137</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820929</td>\n",
       "      <td>0.028169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.774898</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.951626</td>\n",
       "      <td>0.653061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.315508  0.052830  0.573491  0.230769\n",
       "1     rbf support vector machine  0.994805  0.992806  0.999173  0.992248\n",
       "2    poly support vector machine  0.781915  0.820896  0.935510  0.800000\n",
       "3                  decision tree  0.961340  0.882353  0.992126  0.961832\n",
       "4                  random forest  0.956410  0.915493  0.990437  0.961240\n",
       "5                    naive bayes  0.030457  0.000000  0.825137  0.000000\n",
       "6            logistic regression  0.255814  0.000000  0.820929  0.028169\n",
       "7           k nearest neighbours  0.774898  0.563107  0.951626  0.653061"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.459504</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.997396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.738462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.971354</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.989256</td>\n",
       "      <td>0.969231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.971354</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.984298</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920661</td>\n",
       "      <td>0.015385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>0.492308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.460938  0.101449  0.459504  0.230769\n",
       "1     rbf support vector machine  0.997396  1.000000  0.998347  0.984615\n",
       "2    poly support vector machine  0.765625  0.797101  0.947107  0.738462\n",
       "3                  decision tree  0.971354  0.869565  0.989256  0.969231\n",
       "4                  random forest  0.971354  0.942029  0.984298  0.953846\n",
       "5                    naive bayes  0.015625  0.000000  0.998347  0.000000\n",
       "6            logistic regression  0.200521  0.000000  0.920661  0.015385\n",
       "7           k nearest neighbours  0.739583  0.420290  0.991736  0.492308"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.239837</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.762689</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.992228</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.798913</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.924194</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.951531</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.995012</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.941919</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.996653</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.353211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740691</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.813754</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.239837  0.035714  0.762689  0.230769\n",
       "1     rbf support vector machine  0.992228  0.985714  1.000000  1.000000\n",
       "2    poly support vector machine  0.798913  0.846154  0.924194  0.872727\n",
       "3                  decision tree  0.951531  0.895522  0.995012  0.954545\n",
       "4                  random forest  0.941919  0.890411  0.996653  0.968750\n",
       "5                    naive bayes  0.600000  0.000000  0.703143  0.000000\n",
       "6            logistic regression  0.353211  0.000000  0.740691  0.166667\n",
       "7           k nearest neighbours  0.813754  0.852941  0.914634  0.969697"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc       384\n",
       "good       69\n",
       "unacc    1210\n",
       "vgood      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts=df[6].value_counts()\n",
    "pd.Series(map6).map(labels_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 Hidden Markov Model (히든 마르코프 모델)**\n",
    "### **01 은닉 마르코프 함수로 만들기** ([YouTube](https://www.youtube.com/watch?v=gchgZ2zp8uo))\n",
    "- 엄격한 의미의 지도학습은 아니지만, 분류와 매우 비슷 ([HMM](http://shineware.tistory.com/entry/HMM-Hidden-Markov-Model)) | ([YouTube](https://www.youtube.com/watch?v=O1U2NWaSYn4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "def MostLikelyStateSequence(observations):\n",
    "    #calc combinations:\n",
    "    N = self_A.shape[0]\n",
    "    T = len(observations)\n",
    "    sequences = [str(i) for i in range(N)]\n",
    "    probs = np.array([self_pi[i]*self_B[i,observations[0]] for i in range(N)])\n",
    "    print (probs)\n",
    "    for i in range(1,T):\n",
    "        newsequences = []\n",
    "        newprobs = np.array([])\n",
    "        for s in range(len(sequences)):\n",
    "            for j in range(N):\n",
    "                newsequences.append(sequences[s]+str(j))\n",
    "                bef = int(sequences[s][-1])\n",
    "                tTpprob = probs[s]*self_A[bef,j]*self_B[j,observations[i]]\n",
    "                newprobs = np.append(newprobs,[tTpprob]) \n",
    "                print (sequences[s]+str(j),'-',tTpprob)\n",
    "        sequences = newsequences\n",
    "        probs = newprobs\n",
    "    return max((probs[i],sequences[i]) for i in range(len(sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02 비터비 알고리즘**\n",
    "https://www.youtube.com/watch?v=gchgZ2zp8uo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ViterbiSequence(observations):\n",
    "    deltas = [{}]\n",
    "    seq = {}\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    #initialization\n",
    "    for s in states:\n",
    "        deltas[0][s] = self_pi[s]*self_B[s,observations[0]]\n",
    "        seq[s] = [s]\n",
    "    #compute Viterbi\n",
    "    for t in range(1,T):\n",
    "        deltas.append({})\n",
    "        newseq = {}\n",
    "        for s in states:\n",
    "            (delta,state) = max((deltas[t-1][s0]*self_A[s0,s]*self_B[s,observations[t]],s0) for s0 in states)\n",
    "            deltas[t][s] = delta\n",
    "            newseq[s] = seq[state] + [s]\n",
    "        seq = newseq\n",
    "\n",
    "    (delta,state) = max((deltas[T-1][s],s) for s in states)\n",
    "    return  delta,' sequence: ', seq[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03 정확한 상태의 개수를 최대화하는 알고리즘**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxProbSequence(observations):\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    M = self_B.shape[1]\n",
    "    # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "    # Initialize alpha\n",
    "    alpha = np.zeros((N,T))\n",
    "    c = np.zeros(T) #scale factors\n",
    "    alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "    c[0] = 1.0/np.sum(alpha[:,0])\n",
    "    alpha[:,0] = c[0] * alpha[:,0]\n",
    "    # Update alpha for each observation step\n",
    "    for t in range(1,T):\n",
    "        alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "        c[t] = 1.0/np.sum(alpha[:,t])\n",
    "        alpha[:,t] = c[t] * alpha[:,t]\n",
    "\n",
    "    # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "    # Initialize beta\n",
    "    beta = np.zeros((N,T))\n",
    "    beta[:,T-1] = 1\n",
    "    beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "    # Update beta backwards froT end of sequence\n",
    "    for t in range(len(observations)-1,0,-1):\n",
    "        beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "        beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "\n",
    "    norm = np.sum(alpha[:,T-1])\n",
    "    seq = ''\n",
    "    for t in range(T):\n",
    "        g,state = max(((beta[i,t]*alpha[i,t])/norm,i) for i in states)\n",
    "        seq +=str(state)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(time):\n",
    "    def drawFromNormal(probs):\n",
    "        return np.where(np.random.multinomial(1,probs) == 1)[0][0]\n",
    "    observations = np.zeros(time)\n",
    "    states = np.zeros(time)\n",
    "    states[0] = drawFromNormal(self_pi)\n",
    "    observations[0] = drawFromNormal(self_B[states[0],:])\n",
    "    for t in range(1,time):\n",
    "        states[t] = drawFromNormal(self_A[states[t-1],:])\n",
    "        observations[t] = drawFromNormal(self_B[states[t],:])\n",
    "    return observations,states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **04 바움웰치 알고리즘**\n",
    "- 우도를 최대화 하는 파라미터를 찾는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(observations,criterion):\n",
    "    N, M     = self_A.shape[0], self_B.shape[1]\n",
    "    A, B, pi = self_A, self_B, copy(self_pi)\n",
    "    T, convergence = len(observations), False\n",
    "    while not convergence:\n",
    "        alpha = np.zeros((N,T))\n",
    "        c     = np.zeros(T)    #scale factors\n",
    "        alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "        c[0]       = 1.0/np.sum(alpha[:,0])\n",
    "        alpha[:,0] = c[0] * alpha[:,0]\n",
    "        # Update alpha for each observation step\n",
    "        for t in range(1,T):\n",
    "            alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "            c[t]       = 1.0/np.sum(alpha[:,t])\n",
    "            alpha[:,t] = c[t] * alpha[:,t]\n",
    "        #P(O=O_0,O_1,...,O_T-1 | hmm)\n",
    "        P_O = np.sum(alpha[:,T-1])\n",
    "        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "        # Initialize beta\n",
    "        beta = np.zeros((N,T))\n",
    "        beta[:,T-1] = 1\n",
    "        beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "        # Update beta backwards froT end of sequence\n",
    "        for t in range(len(observations)-1,0,-1):\n",
    "            beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "            beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "        gi = np.zeros((N,N,T-1));\n",
    "        for t in range(T-1):\n",
    "            for i in range(N):\n",
    "                gamma_num = alpha[i,t] * self_A[i,:] * \\\n",
    "                    self_B[:,observations[t+1]].T * beta[:,t+1].T\n",
    "                gi[i,:,t] = gamma_num / P_O\n",
    "        # gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "        gamma = np.squeeze(np.sum(gi,axis=1))\n",
    "        # Need final gamma element for new B\n",
    "        prod =  (alpha[:,T-1] * beta[:,T-1]).reshape((-1,1))\n",
    "        gamma_T = prod/P_O\n",
    "        gamma = np.hstack((gamma,  gamma_T)) #append one Tore to gamma!!!\n",
    "        newpi = gamma[:,0]\n",
    "        newA = np.sum(gi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "        newB = copy(B)\n",
    "        sumgamma = np.sum(gamma,axis=1)\n",
    "        for ob_k in range(M):\n",
    "            list_k = observations == ob_k\n",
    "            newB[:,ob_k] = np.sum(gamma[:,list_k],axis=1) / sumgamma\n",
    "        if np.max(abs(pi - newpi)) < criterion and \\\n",
    "               np.max(abs(A - newA)) < criterion and \\\n",
    "               np.max(abs(B - newB)) < criterion:\n",
    "            convergence = True;\n",
    "        A[:],B[:],pi[:] = newA,newB,newpi\n",
    "    self_A[:], self_B[:], self_pi[:] = newA, newB, newpi\n",
    "    self_gamma = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi sequence: (0.004445279999999999, ' sequence: ', [0, 1, 0, 0])\n",
      "max prob sequence: 0100\n"
     ]
    }
   ],
   "source": [
    "pi = np.array([0.6, 0.4])\n",
    "A  = np.array([[0.7, 0.3],      [0.6, 0.4]])\n",
    "B  = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3]])\n",
    "self_pi, self_A, self_B = pi, A, B\n",
    "\n",
    "print ('Viterbi sequence:',ViterbiSequence(np.array([0,1,0,2])))\n",
    "print ('max prob sequence:',maxProbSequence(np.array([0,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated initial probabilities\n",
      " [1. 0.]\n",
      "Estimated state transition probabililities\n",
      " [[0. 1.]\n",
      " [1. 0.]]\n",
      "Estimated observation probabililities\n",
      " [[1.         0.         0.        ]\n",
      " [0.         0.38196618 0.61803382]]\n"
     ]
    }
   ],
   "source": [
    "# obs,states = hmmguess.simulate(4)\n",
    "train(np.array([0,1,0,2]),0.000001)\n",
    "\n",
    "print ('Estimated initial probabilities\\n',pi)\n",
    "print ('Estimated state transition probabililities\\n',A)\n",
    "print ('Estimated observation probabililities\\n',B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
