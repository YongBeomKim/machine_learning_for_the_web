{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **웹 크롤링**\n",
    "Web Mining\n",
    "## **1 Natural Language Processing**\n",
    "자연어 분석을 위한 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "moviehtmldir, moviedict = './data/Movie/', {}\n",
    "\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f  = open(moviehtmldir + '/' + filename, encoding=\"ISO-8859-1\")\n",
    "    parsed_html   = BeautifulSoup(f.read(), \"lxml\")\n",
    "    try:    title = parsed_html.body.h1.text       \n",
    "    except: title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'11962'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c1261e6fad87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdir_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/txt_sentoken/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpos_textreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListDocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'pos/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mneg_textreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListDocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'neg/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtot_textreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_textreviews\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_textreviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c1261e6fad87>\u001b[0m in \u001b[0;36mListDocs\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtitles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoviedict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '11962'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "tknzr    = WordPunctTokenizer()\n",
    "print(len(stoplist))\n",
    "stoplist[::17]\n",
    "\n",
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir_ = './data/txt_sentoken/'\n",
    "pos_textreviews, pos_titles = ListDocs(dir_ + 'pos/')\n",
    "neg_textreviews, neg_titles = ListDocs(dir_ + 'neg/')\n",
    "tot_textreviews = pos_textreviews + neg_textreviews\n",
    "tot_titles      = pos_titles + neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def PreprocessTfidf(texts,stoplist=[],stem=False):\n",
    "    newtexts = []\n",
    "    for text in texts:\n",
    "        if stem:\n",
    "            tmp = [w for w in tknzr.tokenize(text) if w not in stoplist]\n",
    "        else:\n",
    "            tmp = [stemmer.stem(w) for w in [w for w in tknzr.tokenize(text) if w not in stoplist]]\n",
    "        newtexts.append(' '.join(tmp))\n",
    "    return newtexts\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "processed_reviews = PreprocessTfidf(tot_textreviews,stoplist,True)\n",
    "mod_tfidf = vectorizer.fit(processed_reviews)\n",
    "vec_tfidf = mod_tfidf.transform(processed_reviews)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump tf-idf into file\n",
    "import cPickle as pickle\n",
    "#print mod_tfidf.get_feature_names()\n",
    "print len(processed_reviews),'--',len(mod_tfidf.get_feature_names())\n",
    "v= mod_tfidf.transform(processed_reviews)\n",
    "#print v\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "      pickle.dump(mod_tfidf, fin)\n",
    "file = open(\"vectorizer.pk\",'r')\n",
    "load_tfidf =  pickle.load(file)\n",
    "        \n",
    "print load_tfidf.transform(PreprocessTfidf([' '.join(['drama'])],stoplist,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test LSA\n",
    "import gensim\n",
    "from gensim import models\n",
    "class GenSimCorpus(object):\n",
    "    def __init__(self, texts, stoplist=[],stem=False):\n",
    "        self.texts = texts\n",
    "        self.stoplist = stoplist\n",
    "        self.stem = stem\n",
    "        self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __iter__(self):\n",
    "        for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def iter_docs(self,texts, stoplist):\n",
    "        for text in texts:\n",
    "            if self.stem:\n",
    "                yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "            else:\n",
    "                yield (x for x in tknzr.tokenize(text) if x not in stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = GenSimCorpus(tot_textreviews,stoplist,True)\n",
    "dict_corpus = corpus.dictionary\n",
    "ntopics = 10\n",
    "lsi =  models.LsiModel(corpus, num_topics=ntopics, id2word=dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = lsi.projection.u\n",
    "Sigma = np.eye(ntopics)*lsi.projection.s\n",
    "#calculate V\n",
    "V = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "dict_words = {}\n",
    "for i in range(len(dict_corpus)):\n",
    "    dict_words[dict_corpus[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessDoc2Vec(text,stop=[],stem=False):\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Review = namedtuple('Review','words tags')\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "do2vecstem = False\n",
    "reviews_pos = []\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    reviews_pos.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['pos_'+str(cnt)]))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    reviews_neg.append(Review(PreprocessDoc2Vec(f.read(),stoplist,do2vecstem),['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=10, negative=0, hs=0, min_count=1, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query\n",
    "query = ['science','future','action']\n",
    "#similar tfidf\n",
    "#sparse matrix so the metrics transform into regular vectors before computing cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_vec = mod_tfidf.transform(PreprocessTfidf([' '.join(query)],stoplist,True))\n",
    "sims= cosine_similarity(query_vec,vec_tfidf)[0]\n",
    "indxs_sims = sims.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in list(indxs_sims)[:5]:\n",
    "    print 'sim:',sims[d],' title:',tot_titles[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSA query\n",
    "def TransformWordsListtoQueryVec(wordslist,dict_words,stem=False):\n",
    "    q = np.zeros(len(dict_words.keys()))\n",
    "    for w in wordslist:\n",
    "        if stem:\n",
    "            q[dict_words[stemmer.stem(w)]]=1.\n",
    "        else:\n",
    "            q[dict_words[w]] = 1.\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = TransformWordsListtoQueryVec(query,dict_words,True)\n",
    "qk =   np.dot(np.dot(q,U),Sigma)\n",
    "sims = np.zeros(len(tot_textreviews))\n",
    "for d in range(len(V)):\n",
    "    sims[d]=np.dot(qk,V[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indxs_sims = np.argsort(sims)[::-1]  \n",
    "for d in list(indxs_sims)[:5]:\n",
    "    print 'sim:',sims[d],' doc:',tot_titles[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc2vec query\n",
    "#force inference to get the same result\n",
    "model_d2v.random = np.random.RandomState(1)\n",
    "query_docvec = model_d2v.infer_vector(PreprocessDoc2Vec(' '.join(query),stoplist,do2vecstem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_d2v.docvecs.most_similar([query_docvec], topn=3)\n",
    "reviews_related = model_d2v.docvecs.most_similar([query_docvec], topn=5)\n",
    "for review in reviews_related:\n",
    "    print 'relevance:',review[1],'  title:',tot_titles[review[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Post Processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get titles\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "moviehtmldir = './movie/'\n",
    "moviedict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename)\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "        title = parsed_html.body.h1.text\n",
    "    except:\n",
    "        title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = './review_polarity/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "               self.texts = texts\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.bestwords = bestwords\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "            \n",
    "           def __len__(self):\n",
    "               return len(self.texts)\n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:\n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      if len(self.bestwords)>0:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x in self.bestwords)\n",
    "                      else:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x not in stoplist)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda = corpus.dictionary\n",
    "lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "print lda.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#filter out very common words like mobie and film or very unfrequent terms\n",
    "out_ids = [tokenid for tokenid, docfreq in dict_lda.dfs.iteritems() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "corpus = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)\n",
    "for t in range(num_topics):\n",
    "    print 'topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import namedtuple\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PreprocessReviews(text,stop=[],stem=False):\n",
    "    #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "        words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "        words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Review = namedtuple('Review','words title tags')\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['pos_'+str(cnt)]))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['neg_'+str(cnt)]))\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_reviews = reviews_pos + reviews_neg\n",
    "#split in test training sets\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print len(trainfeatures)\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "#shuffle(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "#training naive bayes \n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##testing\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "tot_words = pos_words + neg_words\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word, freq in word_fd.iteritems():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'total: ',len(word_scores)\n",
    "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(trainfeatures)\n",
    "##test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "        err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "train_d2v = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',classifier.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print 'accuracy:',clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svm linear\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
