{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch3 Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 오류 평가\n",
    "- bias : 예측 결과의 편향 (알고리즘의 가정의 문제로 발생)\n",
    "- variance : 분산 (포인트 $X_1$ 에 대한 잘못 예측된 레이블)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Mean Square Error (MSE : 평균 제곱 오차)\n",
    "- 오차 제곱의 평균\n",
    "- 자율학습 방법의 평가척도 </br>\n",
    "<p>$ MSE(\\sigma^-) ={1 \\over N} \\sum d(y_t-y_t^p)^2  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Generalized linear models (일반화 선형모델)\n",
    "\n",
    "- Function ($E_i는 모델의 오차$) : $X$는 1개</br>\n",
    "<p>$ y_i = \\sum \\sigma_j x_j^i + E_i $</br>\n",
    "<p>$ = h_\\sigma(x^i) + E_i $   \n",
    "\n",
    "- Cost : Stochastic Gradient Descent (확률 내리막 경사법)</br>\n",
    "<p>$\\sigma_i$ 값에 따라 최종값 근처의 진폭이 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. linear regression (선형회귀)\n",
    "- 가장 단순한 알고리즘 = 다변량 선형 회귀모델 (univariate liner regression)\n",
    "- 선형 모델 회귀식 : '선형예측함수'를 사용해 회귀식을 모델링, 알려지지 않은 파라미터는 데이터로부터 추정</br>\n",
    "<p> $ h_\\sigma(x^i) = \\sigma_0 + \\sigma_1x_1^i + \\sigma_2x_2^i +...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. ridge regression (리지회귀) \n",
    "- Tikhonov regularization (타코노프 정규화)\n",
    "- 비용함수에 \"정규화 항\"을 추가, 모델에 제약(Constraint)을 부과\n",
    "- 과적합 모델을 방지</br>\n",
    "<p> $ J^` = { 1 \\over 2}\\sum (y_i - h_\\sigma(x^i))^2 + { \\Gamma \\over 2}\\sum_j^2 $</p></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Lasso regression(라소 회귀)\n",
    "- <strong>정규화 항이 \"파라미터 절대값\"의 합</strong> 만 제외한, 나머지는 '04.리지회귀' 와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06. Logistic regression(로지스틱 회귀)\n",
    "- 분류를 위한 '조건부 확률'로 로지스틱 함수를 사용 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) function\n",
    "- Target 값이 범주형 (0,1) 의 값\n",
    "- $ h_\\sigma(x^i) $ 는 0과 1 사이의 연속된 값을 갖는다  : $ h_\\sigma(x^i) $ 는 확률적 분류기(Probabilistic  classifier)\n",
    "<p>$ h_\\sigma(x^i) = \\begin{matrix} > 0.5 : 1\\\\ < 0.5 : 0\\\\\\end{matrix}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Cost function (일반화 선형 모델에 대한 확률적 해석)\n",
    "- 우도를 최대화 하는 것이, 비용함수를 최소화 한다\n",
    "- 내리막 경사법 함께 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07. K-근접 이웃법\n",
    "- 숫자형 데이터간의 거리를 측정\n",
    "- 민코스키 방법(유클리드, 맨해튼 함수를 일반화한 공식) </br>\n",
    "<p>$ (\\sum(|x_j^k - x_j^t|)^q )^{1\\over q}$    ...cf) q = 1 (맨해튼) , 2 (유클리드), 무한대 (슈프림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. 나이브 베이즈 (베이지안 정리)\n",
    "- 나이브 베이즈 (베이지안 정리) : 사후확률을 최대화 해주는 레이블 $P$를 찾는다 <br/>\n",
    "<p> $ P = argmax  P(y|x_0, x_1..... x_M) $  - 종속변수 y 가 최대가 되는 독립변수 x를 찾는 함수</p></br>\n",
    "- 다항 분포 나이브 베이즈 \n",
    "- 가우시안 나이브 베이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09. 의사결정트리\n",
    "- Decision Node : 결정 노드 (2개 이상의 가지가 있고, 의사결정이 나뉠 떄)\n",
    "- Leaf Node : 잎 노드 (데이터를 분류하는 노드)\n",
    "- 최적의 분할 규칙 : <strong>불순도 함수 $I$</strong>를 최소화 하는 것\n",
    "    $$ (t_k^q, q) = argmin.I(t_k^j, j) $$\n",
    "- Cost 함수 \n",
    "    1. Entropy(엔트로피) :  $ H(S_b) = - \\sum P_b \\log_2 P_b $\n",
    "    2. gini impurity(지니불순도) : $ H(S_b) = - \\sum P_b (1- P_b) $\n",
    "    3. misclassification(오분류) : $ H(S_b) = 1 - max(P_b) $\n",
    "    4. mean squared error(평균제곱오차) : $ H(S_b) = { 1 \\over N_b} \\sum (y_i - \\sigma_b)^2 $회귀분석에서 사용\n",
    "- 집합이 클경우 일반화가 잘 안되는 단점 (차원 축소가 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 서포트 벡터 머신\n",
    "- 데이터 포인트에서 결정 경계까지 거리를 최대화\n",
    "- 여백을 최대화 하는 $w, b$의 값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 커널 트릭\n",
    "- 데이터 집합이 선형으로 분리되지 않을 때, 다른 차원의 공간으로 매핑을 한다.\n",
    "- 커널함수 (2차원 분리가능)\n",
    "$$ K(x^i, x^j) = e^{-|x^i-x^j|^2 \\over 2\\sigma^2} $$\n",
    "    1. liner Kernel\n",
    "    2. RBF Basis Kernel\n",
    "    3. Polynomial Kernel\n",
    "    4. Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 방법간의 비교\n",
    "- Cross Validation 절차에 따라 모델을 평가한다\n",
    "  1. k-1개 폴드를 훈련집합으로 모델을 훈련\n",
    "  2. 나머지 1 폴드로 테스트\n",
    "  3. 시작점에 정한 폴드 갯수 k만큼 반복\n",
    "  4. 정확도는 k번 반복으로 얻은 정확도의 평균을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Problem (회귀분석 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델선택 함수/ 0.18이후 버젼에서 통함\n",
    "#from sklearn import cross_validation (통합)\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read_CSV & shuffle the data\n",
    "df = pd.read_csv('./data/housing.data',delim_whitespace=True ,header=None)\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "X = df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values\n",
    "cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression -----  \n",
      "mean R2: 0.70 (+/- 0.20) \n",
      "MSE: 23.3713443396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin = LinearRegression()\n",
    "scores = model_selection.cross_val_score(lin, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lin, X,Y, cv=cv)\n",
    "print ('linear regression ----- ',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression ------  \n",
      "mean R2: 0.70 (+/- 0.20) \n",
      "MSE: 23.5512063519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "scores = model_selection.cross_val_score(ridge, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(ridge, X,Y, cv=cv)\n",
    "print ('ridge regression ------ ',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regression \n",
      "mean R2: 0.68 (+/- 0.21) \n",
      "MSE: 24.634888136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "scores = model_selection.cross_val_score(lasso, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lasso, X,Y, cv=cv)\n",
    "print ('lasso regression',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree regression \n",
      "mean R2: 0.77 (+/- 0.17) \n",
      "MSE: 18.7994664032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "scores = model_selection.cross_val_score(tree, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(tree, X,Y, cv=cv)\n",
    "print ('decision tree regression',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ValueError: min_samples_split must be at least 2 or in (0, 1], got 1\n",
    "#-------------------------------------------------------------------- \n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# forest = RandomForestRegressor(n_estimators=50, max_depth=None,min_samples_split=1, \n",
    "#                                random_state=0)\n",
    "# scores = model_selection.cross_val_score(forest, X, Y, cv=cv)\n",
    "# predicted = model_selection.cross_val_predict(forest, X,Y, cv=cv)\n",
    "# print ('random forest regression',\n",
    "#        \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "#        '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear support vector machine \n",
      "mean R2: 0.68 (+/- 0.17) \n",
      "MSE: 25.9153166441\n"
     ]
    }
   ],
   "source": [
    "svm_lin = svm.SVR(epsilon=0.2,kernel='linear',C=1)\n",
    "scores = model_selection.cross_val_score(svm_lin, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print ('linear support vector machine',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support vector machine rbf \n",
      "mean R2: -0.00 (+/- 0.07) \n",
      "MSE: 83.6274403453\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVR(epsilon=0.2,kernel='rbf',C=1.)\n",
    "scores = model_selection.cross_val_score(clf, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "print ('support vector machine rbf',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn \n",
      "mean R2: 0.52 (+/- 0.19) \n",
      "MSE: 37.9898608696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "scores = model_selection.cross_val_score(knn, X, Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(knn, X,Y, cv=cv)\n",
    "print ('knn',\n",
    "       \"\\nmean R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적 특징 축소방법 (Recursive Feature Elimination Method)\n",
    "- <strong>가장 큰 절대 가중치를 갖는 속성</strong>을 고려하여, 원하는 갯수의 특징을 선택할때까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "best_features=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on linear regression \n",
      "R2: 0.59 (+/- 0.20) \n",
      "MSE: 33.3130886804\n"
     ]
    }
   ],
   "source": [
    "rfe_lin = RFE(lin,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_lin.support_)\n",
    "scores = model_selection.cross_val_score(lin, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lin, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on linear regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection ridge regression \n",
      "R2: 0.59 (+/- 0.19) \n",
      "MSE: 33.381047714\n"
     ]
    }
   ],
   "source": [
    "rfe_ridge = RFE(ridge,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_ridge.support_)\n",
    "scores = model_selection.cross_val_score(ridge, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(ridge, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection ridge regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on lasso regression \n",
      "R2: 0.65 (+/- 0.20) \n",
      "MSE: 27.5840853668\n"
     ]
    }
   ],
   "source": [
    "rfe_lasso = RFE(lasso,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_lasso.support_)\n",
    "scores = model_selection.cross_val_score(lasso, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(lasso, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on lasso regression',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on decision tree \n",
      "R2: 0.70 (+/- 0.19) \n",
      "MSE: 23.1275098814\n"
     ]
    }
   ],
   "source": [
    "rfe_tree = RFE(tree,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_tree.support_)\n",
    "scores = model_selection.cross_val_score(tree, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(tree, X[:,mask],Y, cv=cv)\n",
    "print ('feature selection on decision tree',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection on linear support vector machine \n",
      "R2: 0.58 (+/- 0.21) \n",
      "MSE: 25.9153166441\n"
     ]
    }
   ],
   "source": [
    "rfe_svm = RFE(svm_lin,best_features).fit(X,Y)\n",
    "mask = np.array(rfe_svm.support_)\n",
    "scores = model_selection.cross_val_score(svm_lin, X[:,mask], Y, cv=cv)\n",
    "predicted = model_selection.cross_val_predict(svm_lin, X,Y, cv=cv)\n",
    "print ('feature selection on linear support vector machine',\n",
    "       \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "       '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ValueError: min_samples_split must be at least 2 or in (0, 1], got 1\n",
    "#-------------------------------------------------------------------- \n",
    "\n",
    "# rfe_forest = RFE(forest,best_features).fit(X,Y)\n",
    "# mask = np.array(rfe_forest.support_)\n",
    "# scores = model_selection.cross_val_score(forest, X[:,mask], Y, cv=cv)\n",
    "# predicted = model_selection.cross_val_predict(forest, X[:,mask],Y, cv=cv)\n",
    "# print ('feature selection on random forest',\n",
    "#        \"\\nR2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2),\n",
    "#        '\\nMSE:',mean_squared_error(Y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Problem (분류 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1  2  3      4     5      6\n",
       "0  vhigh  vhigh  2  2  small   low  unacc\n",
       "1  vhigh  vhigh  2  2  small   med  unacc\n",
       "2  vhigh  vhigh  2  2  small  high  unacc\n",
       "3  vhigh  vhigh  2  2    med   low  unacc\n",
       "4  vhigh  vhigh  2  2    med   med  unacc"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data in\n",
    "df = pd.read_csv('./data/car.data', delimiter=',' ,header=None)\n",
    "for i in range(len(df.columns)):\n",
    "    df[i] = df[i].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map catgories to values\n",
    "map0 = dict( zip( df[0].cat.categories, range( len(df[0].cat.categories ))))\n",
    "#print map0\n",
    "map1 = dict( zip( df[1].cat.categories, range( len(df[1].cat.categories ))))\n",
    "map2 = dict( zip( df[2].cat.categories, range( len(df[2].cat.categories ))))\n",
    "map3 = dict( zip( df[3].cat.categories, range( len(df[3].cat.categories ))))\n",
    "map4 = dict( zip( df[4].cat.categories, range( len(df[4].cat.categories ))))\n",
    "map5 = dict( zip( df[5].cat.categories, range( len(df[5].cat.categories ))))\n",
    "map6 = dict( zip( df[6].cat.categories, range( len(df[6].cat.categories ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(['category']).columns\n",
    "df[cat_cols] = df[cat_cols].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6\n",
       "684  0  2  1  1  2  1  2\n",
       "587  0  0  1  2  2  0  0\n",
       "531  0  3  3  2  2  1  2\n",
       "302  3  2  3  0  1  0  2\n",
       "175  3  0  2  1  1  2  2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_f1 = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_precision = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))\n",
    "df_recall = pd.DataFrame(columns=['method']+sorted(map6, key=map6.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def CalcMeasures(method,y_pred,y_true,df_f1=df_f1\n",
    "                 ,df_precision=df_precision,df_recall=df_recall):\n",
    "\n",
    "    df_f1.loc[len(df_f1)] = [method]+list(f1_score(y_pred,y_true,average=None))\n",
    "    df_precision.loc[len(df_precision)] = [method]+list(precision_score(y_pred,y_true,average=None))\n",
    "    df_recall.loc[len(df_recall)] = [method]+list(recall_score(y_pred,y_true,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= df[df.columns[:-1]].values\n",
    "Y = df[df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "#from sklearn import cross_validation (통합)\n",
    "\n",
    "cv = 10\n",
    "method = 'linear support vector machine'\n",
    "clf = svm.SVC(kernel='linear',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'rbf support vector machine'\n",
    "clf = svm.SVC(kernel='rbf',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'poly support vector machine'\n",
    "clf = svm.SVC(kernel='poly',C=50)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "method = 'decision tree'\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "method = 'random forest'\n",
    "clf = RandomForestClassifier(n_estimators=50,random_state=0,max_features=None)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "method = 'naive bayes'\n",
    "clf = MultinomialNB()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/markbaum/Python/django-ml/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "method = 'logistic regression'\n",
    "clf = LogisticRegression()\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "method = 'k nearest neighbours'\n",
    "clf = KNeighborsClassifier(weights='distance',n_neighbors=5)\n",
    "y_pred = model_selection.cross_val_predict(clf, X,Y, cv=cv)\n",
    "CalcMeasures(method,y_pred,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847795</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.997403</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.933878</td>\n",
       "      <td>0.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.956975</td>\n",
       "      <td>0.902256</td>\n",
       "      <td>0.992574</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.959897</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.991722</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.265993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822059</td>\n",
       "      <td>0.054795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.787293</td>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.639175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.262745  0.000000  0.847795  0.000000\n",
       "1     rbf support vector machine  0.997403  0.992701  1.000000  0.992248\n",
       "2    poly support vector machine  0.785714  0.815385  0.933878  0.816667\n",
       "3                  decision tree  0.956975  0.902256  0.992574  0.939394\n",
       "4                  random forest  0.959897  0.920863  0.991722  0.968750\n",
       "5                    naive bayes  0.035354  0.000000  0.825581  0.000000\n",
       "6            logistic regression  0.265993  0.000000  0.822059  0.054795\n",
       "7           k nearest neighbours  0.787293  0.560748  0.949367  0.639175"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.174479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.955729</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.966146</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.990083</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997521</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923967</td>\n",
       "      <td>0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.991736</td>\n",
       "      <td>0.476923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.174479  0.000000  0.985124  0.000000\n",
       "1     rbf support vector machine  1.000000  0.985507  1.000000  0.984615\n",
       "2    poly support vector machine  0.773438  0.768116  0.945455  0.753846\n",
       "3                  decision tree  0.955729  0.869565  0.994215  0.953846\n",
       "4                  random forest  0.966146  0.927536  0.990083  0.953846\n",
       "5                    naive bayes  0.018229  0.000000  0.997521  0.000000\n",
       "6            logistic regression  0.205729  0.000000  0.923967  0.030769\n",
       "7           k nearest neighbours  0.742188  0.434783  0.991736  0.476923"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear support vector machine</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf support vector machine</td>\n",
       "      <td>0.994819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poly support vector machine</td>\n",
       "      <td>0.798387</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.922581</td>\n",
       "      <td>0.890909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decision tree</td>\n",
       "      <td>0.958225</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.925373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.953728</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.993367</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>naive bayes</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704201</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression</td>\n",
       "      <td>0.376190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740397</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest neighbours</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.910470</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          method       acc      good     unacc     vgood\n",
       "0  linear support vector machine  0.531746  0.000000  0.744070  0.000000\n",
       "1     rbf support vector machine  0.994819  1.000000  1.000000  1.000000\n",
       "2    poly support vector machine  0.798387  0.868852  0.922581  0.890909\n",
       "3                  decision tree  0.958225  0.937500  0.990939  0.925373\n",
       "4                  random forest  0.953728  0.914286  0.993367  0.984127\n",
       "5                    naive bayes  0.583333  0.000000  0.704201  0.000000\n",
       "6            logistic regression  0.376190  0.000000  0.740397  0.250000\n",
       "7           k nearest neighbours  0.838235  0.789474  0.910470  0.968750"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc       384\n",
       "good       69\n",
       "unacc    1210\n",
       "vgood      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts=df[6].value_counts()\n",
    "pd.Series(map6).map(labels_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hidden Markov Model (히든 마르코프 모델)\n",
    "- 엄격한 의미의 지도는 아니지만, 분류와 매우 비슷하게 사용된다\n",
    "- http://shineware.tistory.com/entry/HMM-Hidden-Markov-Model\n",
    "- https://www.youtube.com/watch?v=O1U2NWaSYn4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MostLikelyStateSequence(observations):\n",
    "    #calc combinations:\n",
    "    N = self_A.shape[0]\n",
    "    T = len(observations)\n",
    "    sequences = [str(i) for i in range(N)]\n",
    "    probs = np.array([self_pi[i]*self_B[i,observations[0]] for i in range(N)])\n",
    "    print (probs)\n",
    "    for i in range(1,T):\n",
    "        newsequences = []\n",
    "        newprobs = np.array([])\n",
    "        for s in range(len(sequences)):\n",
    "            for j in range(N):\n",
    "                newsequences.append(sequences[s]+str(j))\n",
    "                bef = int(sequences[s][-1])\n",
    "                tTpprob = probs[s]*self_A[bef,j]*self_B[j,observations[i]]\n",
    "                newprobs = np.append(newprobs,[tTpprob]) \n",
    "                print (sequences[s]+str(j),'-',tTpprob)\n",
    "        sequences = newsequences\n",
    "        probs = newprobs\n",
    "    return max((probs[i],sequences[i]) for i in range(len(sequences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. 비터비 알고리즘\n",
    "- https://www.youtube.com/watch?v=gchgZ2zp8uo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ViterbiSequence(observations):\n",
    "    deltas = [{}]\n",
    "    seq = {}\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    #initialization\n",
    "    for s in states:\n",
    "        deltas[0][s] = self_pi[s]*self_B[s,observations[0]]\n",
    "        seq[s] = [s]\n",
    "    #compute Viterbi\n",
    "    for t in range(1,T):\n",
    "        deltas.append({})\n",
    "        newseq = {}\n",
    "        for s in states:\n",
    "            (delta,state) = max((deltas[t-1][s0]*self_A[s0,s]*self_B[s,observations[t]],s0) for s0 in states)\n",
    "            deltas[t][s] = delta\n",
    "            newseq[s] = seq[state] + [s]\n",
    "        seq = newseq\n",
    "\n",
    "    (delta,state) = max((deltas[T-1][s],s) for s in states)\n",
    "    return  delta,' sequence: ', seq[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. 정확한 상태의 개수를 최대화하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxProbSequence(observations):\n",
    "    N = self_A.shape[0]\n",
    "    states = [i for i in range(N)]\n",
    "    T = len(observations)\n",
    "    M = self_B.shape[1]\n",
    "    # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "    # Initialize alpha\n",
    "    alpha = np.zeros((N,T))\n",
    "    c = np.zeros(T) #scale factors\n",
    "    alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "    c[0] = 1.0/np.sum(alpha[:,0])\n",
    "    alpha[:,0] = c[0] * alpha[:,0]\n",
    "    # Update alpha for each observation step\n",
    "    for t in range(1,T):\n",
    "        alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "        c[t] = 1.0/np.sum(alpha[:,t])\n",
    "        alpha[:,t] = c[t] * alpha[:,t]\n",
    "\n",
    "    # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "    # Initialize beta\n",
    "    beta = np.zeros((N,T))\n",
    "    beta[:,T-1] = 1\n",
    "    beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "    # Update beta backwards froT end of sequence\n",
    "    for t in range(len(observations)-1,0,-1):\n",
    "        beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "        beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "\n",
    "    norm = np.sum(alpha[:,T-1])\n",
    "    seq = ''\n",
    "    for t in range(T):\n",
    "        g,state = max(((beta[i,t]*alpha[i,t])/norm,i) for i in states)\n",
    "        seq +=str(state)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(time):\n",
    "    def drawFromNormal(probs):\n",
    "        return np.where(np.random.multinomial(1,probs) == 1)[0][0]\n",
    "    observations = np.zeros(time)\n",
    "    states = np.zeros(time)\n",
    "    states[0] = drawFromNormal(self_pi)\n",
    "    observations[0] = drawFromNormal(self_B[states[0],:])\n",
    "    for t in range(1,time):\n",
    "        states[t] = drawFromNormal(self_A[states[t-1],:])\n",
    "        observations[t] = drawFromNormal(self_B[states[t],:])\n",
    "    return observations,states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. 바움웰치 알고리즘\n",
    "- 우도를 최대화 하는 파라미터를 찾는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(observations,criterion):\n",
    "    N = self_A.shape[0]\n",
    "    T = len(observations)\n",
    "    M = self_B.shape[1]\n",
    "    A = self_A\n",
    "    B = self_B\n",
    "    pi = copy(self_pi)\n",
    "    convergence = False\n",
    "    while not convergence:\n",
    "        # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "        # Initialize alpha\n",
    "        alpha = np.zeros((N,T))\n",
    "        c = np.zeros(T) #scale factors\n",
    "        alpha[:,0] = pi.T * self_B[:,observations[0]]\n",
    "        c[0] = 1.0/np.sum(alpha[:,0])\n",
    "        alpha[:,0] = c[0] * alpha[:,0]\n",
    "        # Update alpha for each observation step\n",
    "        for t in range(1,T):\n",
    "            alpha[:,t] = np.dot(alpha[:,t-1].T, self_A).T * self_B[:,observations[t]]\n",
    "            c[t] = 1.0/np.sum(alpha[:,t])\n",
    "            alpha[:,t] = c[t] * alpha[:,t]\n",
    "        #P(O=O_0,O_1,...,O_T-1 | hmm)\n",
    "        P_O = np.sum(alpha[:,T-1])\n",
    "        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "        # Initialize beta\n",
    "        beta = np.zeros((N,T))\n",
    "        beta[:,T-1] = 1\n",
    "        beta[:,T-1] = c[T-1] * beta[:,T-1]\n",
    "        # Update beta backwards froT end of sequence\n",
    "        for t in range(len(observations)-1,0,-1):\n",
    "            beta[:,t-1] = np.dot(self_A, (self_B[:,observations[t]] * beta[:,t]))\n",
    "            beta[:,t-1] = c[t-1] * beta[:,t-1]\n",
    "        gi = np.zeros((N,N,T-1));\n",
    "        for t in range(T-1):\n",
    "            for i in range(N):\n",
    "                gamma_num = alpha[i,t] * self_A[i,:] * self_B[:,observations[t+1]].T * \\\n",
    "                        beta[:,t+1].T\n",
    "                gi[i,:,t] = gamma_num / P_O\n",
    "        # gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "        gamma = np.squeeze(np.sum(gi,axis=1))\n",
    "        # Need final gamma element for new B\n",
    "        prod =  (alpha[:,T-1] * beta[:,T-1]).reshape((-1,1))\n",
    "        gamma_T = prod/P_O\n",
    "        gamma = np.hstack((gamma,  gamma_T)) #append one Tore to gamma!!!\n",
    "        newpi = gamma[:,0]\n",
    "        newA = np.sum(gi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "        newB = copy(B)\n",
    "        sumgamma = np.sum(gamma,axis=1)\n",
    "        for ob_k in range(M):\n",
    "            list_k = observations == ob_k\n",
    "            newB[:,ob_k] = np.sum(gamma[:,list_k],axis=1) / sumgamma\n",
    "        if np.max(abs(pi - newpi)) < criterion and \\\n",
    "               np.max(abs(A - newA)) < criterion and \\\n",
    "               np.max(abs(B - newB)) < criterion:\n",
    "            convergence = True;\n",
    "        A[:],B[:],pi[:] = newA,newB,newpi\n",
    "    self_A[:] = newA\n",
    "    self_B[:] = newB\n",
    "    self_pi[:] = newpi\n",
    "    self_gamma = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = np.array([0.6, 0.4])\n",
    "A = np.array([[0.7, 0.3],      [0.6, 0.4]])\n",
    "B = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3]])\n",
    "self_pi = pi; self_A = A; self_B = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi sequence: (0.0044452799999999994, ' sequence: ', [0, 1, 0, 0])\n",
      "max prob sequence: 0100\n"
     ]
    }
   ],
   "source": [
    "print ('Viterbi sequence:',ViterbiSequence(np.array([0,1,0,2])))\n",
    "print ('max prob sequence:',maxProbSequence(np.array([0,1,0,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obs,states = hmmguess.simulate(4)\n",
    "train(np.array([0,1,0,2]),0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated initial probabilities\n",
      " [ 1.  0.]\n",
      "Estimated state transition probabililities\n",
      " [[ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "Estimated observation probabililities\n",
      " [[ 1.          0.          0.        ]\n",
      " [ 0.          0.38196618  0.61803382]]\n"
     ]
    }
   ],
   "source": [
    "print ('Estimated initial probabilities\\n',pi)\n",
    "print ('Estimated state transition probabililities\\n',A)\n",
    "print ('Estimated observation probabililities\\n',B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
